# -*- coding: utf-8 -*-
"""project(LR98).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t3MoN6bCfVk3c_rACn0eRv-PCgC_GzRv
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import re
import string
from google.colab import drive

data_fake = pd.read_csv('/content/drive/MyDrive/fakenewsdataset(unprocessed)/fake.csv')  # Replace 'output.csv' with your file name
data_true = pd.read_csv('/content/drive/MyDrive/fakenewsdataset(unprocessed)/true.csv')  # Replace 'output.csv' with your file name

data_fake.head()

data_true.head()

data_fake["class"] = 0
data_true["class"] = 1

data_fake.shape , data_true.shape

data_fake_manual_testing = data_fake.tail(10)
for i in range(23480, 23470, -1):
    data_fake.drop([i], axis = 0 , inplace = True)

data_true_manual_testing = data_true.tail(10)
for i in range(21416, 21406, -1):
    data_true.drop([i], axis = 0 , inplace = True)

data_fake.shape, data_true.shape

data_fake_manual_testing['class'] = 0
data_true_manual_testing['class'] = 1

data_fake_manual_testing.head()

data_true_manual_testing.head()

data_merge = pd.concat([data_fake, data_true], axis = 0)
data_merge.head(10)

data_merge.columns

data_merge.to_csv('output.csv', index=False)

data = data_merge.drop(['title', 'subject','date' ], axis = 1)

data.isnull().sum()

data = data.sample(frac = 1)
#shuffling of data

data.head()

data.reset_index(inplace= True)
data.drop(['index'], axis = 1, inplace = True)

data.columns

data.head()

data_merge.to_csv('final_output.csv', index=False)

# Step 1: Load preprocessed dataset
import pandas as pd

df = pd.read_csv('final_output.csv')  # Replace 'output.csv' with your file name
# Step 2: No code needed as it's described in the step

# Step 3: Clean the noise using NLP libraries (NLTK and SAFAR v2)
import nltk
from nltk.corpus import stopwords


nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def clean_text(text):
    # Remove punctuation
    text = text.replace('.', '').replace(',', '').replace('"', '').replace("'", "")



    # Remove stopwords
    tokens = [word for word in text.split() if word.lower() not in stop_words]

    return ' '.join(tokens)

df['clean_text'] = df['text'].apply(clean_text)

# Step 4: Perform feature extraction
df['word_count'] = df['clean_text'].apply(lambda x: len(x.split()))
df['avg_word_length'] = df['clean_text'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if len(x.split()) != 0 else 0)
df['article_length'] = df['clean_text'].apply(len)
df['number_count'] = df['clean_text'].apply(lambda x: sum(c.isdigit() for c in x))


# Assuming you have already calculated the features and stored them in the DataFrame df

# Create a new DataFrame to store the features
feature_df = pd.DataFrame({
    'Word Count': df['word_count'],
    'Average Word Length': df['avg_word_length'],
    'Article Length': df['article_length'],
    'Number Count': df['number_count']
})

# Print the feature DataFrame
print(feature_df)

# Step 6: Split dataset into train and test sets
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# Assuming 'clean_text' column is already created in your DataFrame df
X_text = df['clean_text']
y = df['class']

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the text data
X_tfidf = tfidf_vectorizer.fit_transform(X_text)

# Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3)

# Step 7: Produce classification model
from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression()
classifier.fit(X_train, y_train)

# Step 8: Test model precision and produce confusion matrix
from sklearn.metrics import precision_score, confusion_matrix
from sklearn.metrics import accuracy_score

y_pred = classifier.predict(X_test)
precision = precision_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print("Precision logistic Regression:", precision)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy Logistic Regression:", accuracy)

print("Confusion Matrix:")
print(conf_matrix)

from sklearn.metrics import f1_score
print('precision :')
print('weighted::')
print(f1_score(y_test, y_pred, average='weighted'))
print('macro::')
print(f1_score(y_test, y_pred, average='macro'))

from sklearn.metrics import recall_score
recall = recall_score(y_test, y_pred, average='weighted')
print(recall)

# Plotting the heatmap
import seaborn as sns
labels = ['fake', 'real']
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)

plt.title('Confusion Matrix Logistic Regression')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Initialize Logistic Regression classifier
logistic_classifier = LogisticRegression()

# Train the classifier
logistic_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred_logistic = logistic_classifier.predict(X_test)

# Generate classification report
report_logistic = classification_report(y_test, y_pred_logistic)

# Print the classification report
print("Classification Report for Logistic Regression:")
print(report_logistic)

