# -*- coding: utf-8 -*-
"""FakeNews_Naivebayes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HP62POPzsLqOtKaB_YnBewc02EH1JTeY
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import re
import string
from google.colab import drive

# Step 1: Load preprocessed dataset
import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/fake news/output_fakenews.csv')  # Replace 'output.csv' with your file name
# Step 2: No code needed as it's described in the step

# Step 3: Clean the noise using NLP libraries (NLTK and SAFAR v2)
import nltk
from nltk.corpus import stopwords


nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def clean_text(text):
    # Remove punctuation
    text = text.replace('.', '').replace(',', '').replace('"', '').replace("'", "")



    # Remove stopwords
    tokens = [word for word in text.split() if word.lower() not in stop_words]

    return ' '.join(tokens)

df['clean_text'] = df['text'].apply(clean_text)

# Step 4: Perform feature extraction
df['word_count'] = df['clean_text'].apply(lambda x: len(x.split()))
df['avg_word_length'] = df['clean_text'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if len(x.split()) != 0 else 0)
df['article_length'] = df['clean_text'].apply(len)
df['number_count'] = df['clean_text'].apply(lambda x: sum(c.isdigit() for c in x))


# Assuming you have already calculated the features and stored them in the DataFrame df

# Create a new DataFrame to store the features
feature_df = pd.DataFrame({
    'Word Count': df['word_count'],
    'Average Word Length': df['avg_word_length'],
    'Article Length': df['article_length'],
    'Number Count': df['number_count']
})

# Print the feature DataFrame
print(feature_df)

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
# Step 6: Split dataset into train and test sets
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score
# Assuming 'clean_text' column is already created in your DataFrame df
X_text = df['clean_text']
y = df['class']

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the text data
X_tfidf = tfidf_vectorizer.fit_transform(X_text)


# Assuming X_tfidf is already defined as the feature matrix using TF-IDF
X = X_tfidf
y = df['class']

# Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Initialize Naive Bayes classifier
nb_classifier = MultinomialNB()

# Train the classifier
nb_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = nb_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)

print("Accuracy of Naive Bayes model:", accuracy)
print("Precision:", precision)

# Step 8: Test model precision and produce confusion matrix
from sklearn.metrics import confusion_matrix
y_pred = nb_classifier.predict(X_test)

conf_matrix = confusion_matrix(y_test, y_pred)


print("confusion matrix")
print(conf_matrix)

from sklearn.metrics import classification_report

# Generate classification report
report = classification_report(y_test, y_pred)

# Print the classification report
print("Classification Report(Naive Bayes):")
print(report)

# Plotting the heatmap
import seaborn as sns
labels = ['fake', 'real']
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)

plt.title('Confusion Matrix naive bayes')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

