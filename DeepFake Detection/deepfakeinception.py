# -*- coding: utf-8 -*-
"""deepFakeInception.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17cL-456OI7yCXnVZcbmwQ0EENHRXLLM4

# DeepFake Detection Model - Inception

Importing libraries in oder build the model
"""

import numpy as np
import matplotlib.pyplot as plt
import itertools
import tensorflow as tf
from sklearn.metrics import confusion_matrix
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model

"""Defining the image dimensions in the acceptable for the Incpetion V3 model and also defining the batch size"""

# Define image dimensions
img_height, img_width = 299, 299  # InceptionV3 input size
batch_size = 32

"""Function in order to define the subset of the dataset as the entire dataset is very large and then loading the datasets for Training.."""

# Function to load a subset of data from the dataset
def load_data_subset(directory, subset_percentage, target_size=(img_height, img_width)):
    datagen = ImageDataGenerator(rescale=1./255, validation_split=1.0 - subset_percentage)
    generator = datagen.flow_from_directory(
        directory,
        target_size=target_size,
        batch_size=batch_size,
        class_mode='binary',
        shuffle=True,
        subset='training'
    )
    return generator

# Define subset percentage
subset_percentage = 0.3  # Change this to desired percentage

# Load subset of training and validation data
train_generator = load_data_subset('/content/drive/MyDrive/Dataset/Train', subset_percentage)
val_generator = load_data_subset('/content/drive/MyDrive/Dataset/Validation', subset_percentage)

"""Load pre-trained InceptionV3 model and giving the input shape and keeping the trained model same"""

# Load pre-trained InceptionV3 model
base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))

# Freeze the convolutional base
base_model.trainable = False

# Add custom classification layers
x = GlobalAveragePooling2D()(base_model.output)
x = Dense(128, activation='relu')(x)
predictions = Dense(1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

"""Actual training occurs here..."""

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    epochs=5,
    validation_data=val_generator,
    validation_steps=val_generator.samples // batch_size
)

"""Evaluation of the model..."""

# Evaluate the model
loss, accuracy = model.evaluate(val_generator)
print("Accuracy:", accuracy)

"""# Improved implementation"""

import numpy as np
import matplotlib.pyplot as plt
import itertools
import tensorflow as tf
from sklearn.metrics import confusion_matrix
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.models import Model

# Define image dimensions
img_height, img_width = 299, 299  # InceptionV3 input size
batch_size = 32

# Function to load a subset of data from the dataset
def load_data_subset(directory, subset_percentage, target_size=(img_height, img_width)):
    datagen = ImageDataGenerator(rescale=1./255, validation_split=1.0 - subset_percentage)
    generator = datagen.flow_from_directory(
        directory,
        target_size=target_size,
        batch_size=batch_size,
        class_mode='binary',
        shuffle=True,
        subset='training'
    )
    return generator

# Define subset percentage
subset_percentage = 0.4  # Change this to desired percentage

# Load subset of training and validation data
train_generator = load_data_subset('/content/drive/MyDrive/Dataset/Train', subset_percentage)
val_generator = load_data_subset('/content/drive/MyDrive/Dataset/Validation', subset_percentage)

# Load pre-trained InceptionV3 model
base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))

# Freeze the convolutional base
base_model.trainable = False

# Add custom classification layers
x = GlobalAveragePooling2D()(base_model.output)
x = Dense(128, activation='relu')(x)
x = Dense(64, activation='relu')(x)
x = Dropout(0.5)(x)  # Add Dropout layer for regularization
predictions = Dense(1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    epochs=10,
    validation_data=val_generator,
    validation_steps=val_generator.samples // batch_size
)

# Evaluate the model
loss, accuracy = model.evaluate(val_generator)
print("Accuracy:", accuracy)

# Fine-tune the model
base_model.trainable = True
fine_tune_at = 200  # Fine-tune from this layer onwards
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False
optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
history_fine_tune = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    epochs=20,  # Adjust epochs as needed
    validation_data=val_generator,
    validation_steps=val_generator.samples // batch_size
)

model.summary()

# Generate predictions
val_generator.reset()
y_pred = model.predict(val_generator)
y_pred = np.round(y_pred).reshape(-1)

# Get true labels
y_true = val_generator.classes

+# Generate confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(2)  # Assuming only two classes (Real and Fake)
plt.xticks(tick_marks, ['Real', 'Fake'])
plt.yticks(tick_marks, ['Real', 'Fake'])

thresh = cm.max() / 2.
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, format(cm[i, j], 'd'),
             horizontalalignment="center",
             color="white" if cm[i, j] > thresh else "black")

plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

import os
import random
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image

# Define the paths to the real and fake image folders
real_folder = '/content/drive/MyDrive/real/'
fake_folder = '/content/drive/MyDrive/fake/'

# Generate a random image ID
image_id = random.randint(0, 99)

# Choose whether to load a real or fake image
is_real = random.choice([True, False])

# Choose the folder based on the image type
image_folder = real_folder if is_real else fake_folder

# Construct the image path
img_path = os.path.join(image_folder, f'{image_id}.jpg')

# Load and preprocess the image
img = image.load_img(img_path, target_size=(img_height, img_width))
img_array = image.img_to_array(img)
img_array = tf.expand_dims(img_array, 0)  # Expand the shape to match the batch size
img_array /= 255.0  # Normalize the pixel values

# Get the model prediction
prediction = model.predict(img_array)
predicted_label = "Fake" if prediction[0][0] > 0.5 else "Real"

# Display the image and predicted label
plt.imshow(img)
plt.title(f'Predicted Label: {predicted_label}')
plt.axis('off')
plt.show()

x`import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions
import numpy as np
import matplotlib.pyplot as plt

# Load the pretrained VGG16 model
model = tf.keras.applications.VGG16(weights='imagenet')

# Load and preprocess the image
img_path = '/content/drive/MyDrive/Dataset/Test/Fake/fake_1009.jpg'
img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

# Run inference
preds = model.predict(x)

# Decode and print the top-3 predicted classes
decoded_preds = decode_predictions(preds, top=3)[0]
print('Predicted:', decoded_preds)

# Display the image and its predicted labels
plt.figure(figsize=(8, 8))
plt.imshow(img)
plt.axis('off')
plt.title('Predicted Labels:')
for i, (imagenet_id, label, score) in enumerate(decoded_preds):
    plt.text(0, i * 20 + 20, f'{label}: {score:.2f}', fontsize=12, color='white', backgroundcolor='blue')
plt.show()

